{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['Left Precentral Gyrus',\n",
    " 'RightPrecentral Gyrus',\n",
    " 'Left Superior Frontal Gyrus',\n",
    " 'Right Superior Frontal Gyrus',\n",
    " 'Left Superior Orbital Gyrus',\n",
    " 'Right Superior Orbital Gyrus',\n",
    " 'Left Middle Frontal Gyrus',\n",
    " 'Right Middle Frontal Gyrus',\n",
    " 'Left Middle Orbital Gyrus',\n",
    " 'Right Middle Orbital Gyrus',\n",
    " 'Left Inferior Frontal Gyrus (p. Opercularis)',\n",
    " 'Right Inferior Frontal Gyrus (p. Opercularis)',\n",
    " 'Left Inferior Frontal Gyrus (p. Triangularis)',\n",
    " 'Right Inferior Frontal Gyrus (p. Triangularis)',\n",
    " 'Left Inferior Frontal Gyrus (p. Orbitalis)',\n",
    " 'Right Inferior Frontal Gyrus (p. Orbitalis)',\n",
    " 'Left Rolandic Operculum',\n",
    " 'Right Rolandic Operculum',\n",
    " 'Left SMA',\n",
    " 'Right SMA',\n",
    " 'Left Olfactory cortex',\n",
    " 'Right Olfactory cortex',\n",
    " 'Left Superior Medial Gyrus',\n",
    " 'Right Superior Medial Gyrus',\n",
    " 'Left Mid Orbital Gyrus',\n",
    " 'Right Mid Orbital Gyrus',\n",
    " 'Left Rectal Gyrus',\n",
    " 'Right Rectal Gyrus',\n",
    " 'Left Insula Lobe',\n",
    " 'Right Insula Lobe',\n",
    " 'Left Anterior Cingulate Cortex',\n",
    " 'Right Anterior Cingulate Cortex',\n",
    " 'Left Middle Cingulate Cortex',\n",
    " 'Right Middle Cingulate Cortex',\n",
    " 'Left Posterior Cingulate Cortex',\n",
    " 'Right Posterior Cingulate Cortex',\n",
    " 'Left Hippocampus',\n",
    " 'Right Hippocampus',\n",
    " 'Left ParaHippocampal Gyrus',\n",
    " 'Right ParaHippocampal Gyrus',\n",
    " 'Left Amygdala',\n",
    " 'Right Amygdala',\n",
    " 'Left Calcarine Gyrus',\n",
    " 'Right Calcarine Gyrus',\n",
    " 'Left Cuneus',\n",
    " 'Right Cuneus',\n",
    " 'Left Lingual Gyrus',\n",
    " 'Right Lingual Gyrus',\n",
    " 'Left Superior Occipital Gyrus',\n",
    " 'Right Superior Occipital Gyrus',\n",
    " 'Left Middle Occipital Gyrus',\n",
    " 'Right Middle Occipital Gyrus',\n",
    " 'Left Inferior Occipital Gyrus',\n",
    " 'Right Inferior Occipital Gyrus',\n",
    " 'Left Fusiform Gyrus',\n",
    " 'Right Fusiform Gyrus',\n",
    " 'Left Postcentral Gyrus',\n",
    " 'Right Postcentral Gyrus',\n",
    " 'Left Superior Parietal Lobule ',\n",
    " 'Right Superior Parietal Lobule ',\n",
    " 'Left Inferior Parietal Lobule ',\n",
    " 'Right Inferior Parietal Lobule ',\n",
    " 'Left SupraMarginal Gyrus',\n",
    " 'Right SupraMarginal Gyrus',\n",
    " 'Left Angular Gyrus',\n",
    " 'Right Angular Gyrus',\n",
    " 'Left Precuneus',\n",
    " 'Right Precuneus',\n",
    " 'Left Paracentral Lobule',\n",
    " 'Right Paracentral Lobule',\n",
    " 'Left Caudate Nucleus',\n",
    " 'Right Caudate Nucleus',\n",
    " 'Left Putamen',\n",
    " 'Right Putamen',\n",
    " 'Left Pallidum',\n",
    " 'Right Pallidum',\n",
    " 'Left Thalamus',\n",
    " 'Right Thalamus',\n",
    " 'Left Heschls Gyrus',\n",
    " 'Right Heschls Gyrus',\n",
    " 'Left Superior Temporal Gyrus',\n",
    " 'Right Superior Temporal Gyrus',\n",
    " 'Left Temporal Pole',\n",
    " 'Right Temporal Pole',\n",
    " 'Left Middle Temporal Gyrus',\n",
    " 'Right Middle Temporal Gyrus',\n",
    " 'Left Medial Temporal Pole',\n",
    " 'Right Medial Temporal Pole',\n",
    " 'Left Inferior Temporal Gyrus',\n",
    " 'Right Inferior Temporal Gyrus',\n",
    " 'Left Cerebellum',\n",
    " 'Right Cerebellum',\n",
    " 'Left Cerebellum',\n",
    " 'Right Cerebellum',\n",
    " 'Left Cerebellum',\n",
    " 'Right Cerebellum',\n",
    " 'Left Cerebellum',\n",
    " 'Right Cerebellum',\n",
    " 'Left Cerebellum',\n",
    " 'Right Cerebellum',\n",
    " 'Left Cerebellum',\n",
    " 'Right Cerebellum',\n",
    " 'Left Cerebellum',\n",
    " 'Right Cerebellum',\n",
    " 'Left Cerebellum',\n",
    " 'Right Cerebellum',\n",
    " 'Left Cerebellum',\n",
    " 'Right Cerebellum',\n",
    " 'Cerebellar Vermis',\n",
    " 'Cerebellar Vermis',\n",
    " 'Cerebellar Vermis',\n",
    " 'Cerebellar Vermis',\n",
    " 'Cerebellar Vermis',\n",
    " 'Cerebellar Vermis',\n",
    " 'Cerebellar Vermis',\n",
    " 'Cerebellar Vermis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from nilearn.datasets import fetch_abide_pcp\n",
    "\n",
    "abide_data=fetch_abide_pcp(\n",
    "                            pipeline=\"cpac\",\n",
    "                            derivatives=[\"rois_ez\"],\n",
    "                            data_dir=\"data\"\n",
    "                            )\n",
    "\n",
    "\n",
    "all_signals=abide_data[\"rois_ez\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "connectome = ConnectivityMeasure(kind='correlation')\n",
    "p_corr_matrices=connectome.fit_transform(all_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>196.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>196.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.001450</td>\n",
       "      <td>0.022857</td>\n",
       "      <td>-0.036227</td>\n",
       "      <td>-0.099389</td>\n",
       "      <td>-0.104059</td>\n",
       "      <td>-0.030562</td>\n",
       "      <td>-0.053539</td>\n",
       "      <td>-0.178875</td>\n",
       "      <td>-0.108470</td>\n",
       "      <td>-0.183873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007383</td>\n",
       "      <td>-0.128369</td>\n",
       "      <td>-0.227351</td>\n",
       "      <td>0.069615</td>\n",
       "      <td>-0.021158</td>\n",
       "      <td>0.319485</td>\n",
       "      <td>0.065955</td>\n",
       "      <td>0.261318</td>\n",
       "      <td>-0.022432</td>\n",
       "      <td>0.038710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.485831</td>\n",
       "      <td>12.331253</td>\n",
       "      <td>9.315370</td>\n",
       "      <td>9.428494</td>\n",
       "      <td>6.571286</td>\n",
       "      <td>5.254639</td>\n",
       "      <td>7.757149</td>\n",
       "      <td>13.696028</td>\n",
       "      <td>9.614269</td>\n",
       "      <td>12.739541</td>\n",
       "      <td>...</td>\n",
       "      <td>13.308686</td>\n",
       "      <td>16.949831</td>\n",
       "      <td>66.946950</td>\n",
       "      <td>48.918267</td>\n",
       "      <td>69.286378</td>\n",
       "      <td>27.842423</td>\n",
       "      <td>27.324602</td>\n",
       "      <td>32.005531</td>\n",
       "      <td>41.562070</td>\n",
       "      <td>63.609655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-25.540736</td>\n",
       "      <td>-33.191009</td>\n",
       "      <td>-27.560425</td>\n",
       "      <td>-22.475268</td>\n",
       "      <td>-15.002795</td>\n",
       "      <td>-13.656543</td>\n",
       "      <td>-18.865641</td>\n",
       "      <td>-32.376979</td>\n",
       "      <td>-23.528885</td>\n",
       "      <td>-28.789255</td>\n",
       "      <td>...</td>\n",
       "      <td>-28.349676</td>\n",
       "      <td>-40.688294</td>\n",
       "      <td>-160.555930</td>\n",
       "      <td>-110.897012</td>\n",
       "      <td>-159.473380</td>\n",
       "      <td>-61.957686</td>\n",
       "      <td>-60.099262</td>\n",
       "      <td>-73.309302</td>\n",
       "      <td>-118.478647</td>\n",
       "      <td>-145.557051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-9.225960</td>\n",
       "      <td>-8.075936</td>\n",
       "      <td>-6.419223</td>\n",
       "      <td>-6.544401</td>\n",
       "      <td>-4.291697</td>\n",
       "      <td>-3.452093</td>\n",
       "      <td>-5.639726</td>\n",
       "      <td>-8.888012</td>\n",
       "      <td>-6.950410</td>\n",
       "      <td>-10.445441</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.190252</td>\n",
       "      <td>-10.238868</td>\n",
       "      <td>-46.289337</td>\n",
       "      <td>-32.288542</td>\n",
       "      <td>-49.777165</td>\n",
       "      <td>-19.782691</td>\n",
       "      <td>-19.488540</td>\n",
       "      <td>-21.668236</td>\n",
       "      <td>-27.598052</td>\n",
       "      <td>-41.539887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.822121</td>\n",
       "      <td>-0.595621</td>\n",
       "      <td>0.080883</td>\n",
       "      <td>-0.827978</td>\n",
       "      <td>-0.382441</td>\n",
       "      <td>0.110651</td>\n",
       "      <td>-0.784757</td>\n",
       "      <td>-1.337740</td>\n",
       "      <td>-0.439015</td>\n",
       "      <td>0.042260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225791</td>\n",
       "      <td>0.160582</td>\n",
       "      <td>-8.645033</td>\n",
       "      <td>0.191204</td>\n",
       "      <td>2.074684</td>\n",
       "      <td>2.083247</td>\n",
       "      <td>-0.573553</td>\n",
       "      <td>-1.327220</td>\n",
       "      <td>0.572747</td>\n",
       "      <td>-4.140064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.024406</td>\n",
       "      <td>8.528740</td>\n",
       "      <td>6.578071</td>\n",
       "      <td>7.211766</td>\n",
       "      <td>3.797011</td>\n",
       "      <td>2.930121</td>\n",
       "      <td>5.566430</td>\n",
       "      <td>9.485969</td>\n",
       "      <td>6.909476</td>\n",
       "      <td>8.836242</td>\n",
       "      <td>...</td>\n",
       "      <td>9.362595</td>\n",
       "      <td>9.608261</td>\n",
       "      <td>43.680708</td>\n",
       "      <td>34.563455</td>\n",
       "      <td>52.462330</td>\n",
       "      <td>19.677259</td>\n",
       "      <td>19.580062</td>\n",
       "      <td>19.298726</td>\n",
       "      <td>27.146490</td>\n",
       "      <td>38.181168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>30.289503</td>\n",
       "      <td>31.314284</td>\n",
       "      <td>27.525428</td>\n",
       "      <td>25.076597</td>\n",
       "      <td>17.754688</td>\n",
       "      <td>14.136722</td>\n",
       "      <td>21.078114</td>\n",
       "      <td>37.845215</td>\n",
       "      <td>20.650393</td>\n",
       "      <td>27.823105</td>\n",
       "      <td>...</td>\n",
       "      <td>33.970576</td>\n",
       "      <td>57.266565</td>\n",
       "      <td>141.569095</td>\n",
       "      <td>118.069203</td>\n",
       "      <td>169.371943</td>\n",
       "      <td>61.883150</td>\n",
       "      <td>67.088593</td>\n",
       "      <td>89.719565</td>\n",
       "      <td>123.707480</td>\n",
       "      <td>152.209138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5    \\\n",
       "count  196.000000  196.000000  196.000000  196.000000  196.000000  196.000000   \n",
       "mean    -0.001450    0.022857   -0.036227   -0.099389   -0.104059   -0.030562   \n",
       "std     12.485831   12.331253    9.315370    9.428494    6.571286    5.254639   \n",
       "min    -25.540736  -33.191009  -27.560425  -22.475268  -15.002795  -13.656543   \n",
       "25%     -9.225960   -8.075936   -6.419223   -6.544401   -4.291697   -3.452093   \n",
       "50%     -0.822121   -0.595621    0.080883   -0.827978   -0.382441    0.110651   \n",
       "75%      9.024406    8.528740    6.578071    7.211766    3.797011    2.930121   \n",
       "max     30.289503   31.314284   27.525428   25.076597   17.754688   14.136722   \n",
       "\n",
       "              6           7           8           9    ...         106  \\\n",
       "count  196.000000  196.000000  196.000000  196.000000  ...  196.000000   \n",
       "mean    -0.053539   -0.178875   -0.108470   -0.183873  ...    0.007383   \n",
       "std      7.757149   13.696028    9.614269   12.739541  ...   13.308686   \n",
       "min    -18.865641  -32.376979  -23.528885  -28.789255  ...  -28.349676   \n",
       "25%     -5.639726   -8.888012   -6.950410  -10.445441  ...   -8.190252   \n",
       "50%     -0.784757   -1.337740   -0.439015    0.042260  ...   -0.225791   \n",
       "75%      5.566430    9.485969    6.909476    8.836242  ...    9.362595   \n",
       "max     21.078114   37.845215   20.650393   27.823105  ...   33.970576   \n",
       "\n",
       "              107         108         109         110         111         112  \\\n",
       "count  196.000000  196.000000  196.000000  196.000000  196.000000  196.000000   \n",
       "mean    -0.128369   -0.227351    0.069615   -0.021158    0.319485    0.065955   \n",
       "std     16.949831   66.946950   48.918267   69.286378   27.842423   27.324602   \n",
       "min    -40.688294 -160.555930 -110.897012 -159.473380  -61.957686  -60.099262   \n",
       "25%    -10.238868  -46.289337  -32.288542  -49.777165  -19.782691  -19.488540   \n",
       "50%      0.160582   -8.645033    0.191204    2.074684    2.083247   -0.573553   \n",
       "75%      9.608261   43.680708   34.563455   52.462330   19.677259   19.580062   \n",
       "max     57.266565  141.569095  118.069203  169.371943   61.883150   67.088593   \n",
       "\n",
       "              113         114         115  \n",
       "count  196.000000  196.000000  196.000000  \n",
       "mean     0.261318   -0.022432    0.038710  \n",
       "std     32.005531   41.562070   63.609655  \n",
       "min    -73.309302 -118.478647 -145.557051  \n",
       "25%    -21.668236  -27.598052  -41.539887  \n",
       "50%     -1.327220    0.572747   -4.140064  \n",
       "75%     19.298726   27.146490   38.181168  \n",
       "max     89.719565  123.707480  152.209138  \n",
       "\n",
       "[8 rows x 116 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_signals[0]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation version\n",
    "features=[]\n",
    "num_rois=116\n",
    "num_features=3\n",
    "for i,signal in enumerate(all_signals):\n",
    "    #m=np.mean(signal)\n",
    "    #s=np.std(signal)\n",
    "    #all_signals[i]=(signal-m)/s\n",
    "    features_matrix=np.empty((num_rois,num_features))\n",
    "    miu=all_signals[i].mean(axis=0) #mean of signal\n",
    "    sd=all_signals[i].std(axis=0) #standard deviation\n",
    "    features_matrix[:,1]=miu\n",
    "    features_matrix[:,-1]=sd\n",
    "\n",
    "    #calculating activation rate of each region\n",
    "    act_rate_list=[]\n",
    "    for r in range(num_rois):\n",
    "        time_series=all_signals[i][:,r]\n",
    "        act_rate_list+=[len(time_series[time_series>miu[r]])/(all_signals[i].shape[0])] #miu[r] is the threshold\n",
    "\n",
    "    act_rate_list=np.array(act_rate_list)\n",
    "    features_matrix[:,0]=act_rate_list\n",
    "\n",
    "    features.append(features_matrix)\n",
    "\n",
    "    np.fill_diagonal(p_corr_matrices[i],0)\n",
    "    p_corr_matrices[i][(p_corr_matrices[i]>-0.3)&(p_corr_matrices[i]<0.3)]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partial correlation version\n",
    "cov_matrices=[]\n",
    "for i,signal in enumerate(all_signals):\n",
    "    m=np.mean(signal)\n",
    "    s=np.std(signal)\n",
    "    all_signals[i]=(signal-m)/s\n",
    "    cov_matrices.append(np.cov(signal.T))\n",
    "    np.fill_diagonal(p_corr_matrices[i],0)\n",
    "    p_corr_matrices[i][(p_corr_matrices[i]>-0.1)&(p_corr_matrices[i]<0.1)]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=np.array(list(abide_data[\"phenotypic\"][\"DX_GROUP\"]))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "dataset=[]\n",
    "\n",
    "for i in range(len(features)):\n",
    "    x=torch.tensor(features[i],requires_grad=True,dtype=torch.float32)\n",
    "    edge_index,edge_weight=dense_to_sparse(torch.tensor(p_corr_matrices[i],dtype=torch.float32))\n",
    "\n",
    "    y=torch.tensor([targets[i]])\n",
    "\n",
    "    dataset.append(Data(x,edge_index,edge_weight,y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set= train_test_split(dataset, test_size=0.3, random_state=42,stratify=targets)\n",
    "val_set, test_set=train_test_split(test_set, test_size=0.5,random_state=42)\n",
    "batch_size=32\n",
    "train_loader=DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader=DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "val_loader=DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Attention model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GlobalAttention,global_mean_pool,GATConv\n",
    "\n",
    "\n",
    "class GATClassifier(nn.Module):\n",
    "    def __init__(self, heads=8 ,out_features=8,p=0.1, hidden_size1=16,hidden_size2=8,in_features=116, num_nodes=116):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_classes = 2\n",
    "        self.dropout = p\n",
    "        self.in_features=in_features\n",
    "        self.out_features=out_features\n",
    "        #self.gate_nn=torch.nn.Linear(, 1)\n",
    "\n",
    "        # Define GCN layers\n",
    "        self.conv1 = GATConv(in_features, out_features, heads=heads)\n",
    "        self.conv2 = GATConv(out_features*heads, out_features)\n",
    "        \n",
    "\n",
    "        # Fully connected layer for flattened node features\n",
    "        self.fc1 = nn.Linear(out_features, hidden_size1)\n",
    "        self.fc2=nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3=nn.Linear(hidden_size2, self.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight, batch):\n",
    "        # GCN layers with ReLU activation\n",
    "        out = F.relu(self.conv1(x, edge_index, edge_weight))\n",
    "\n",
    "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "\n",
    "        out = F.relu(self.conv2(out, edge_index, edge_weight))\n",
    "\n",
    "        \n",
    "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # Apply dropout\n",
    "        #global_attention_pool=GlobalAttention(self.gate_nn)\n",
    "        out = global_mean_pool(out,batch)\n",
    "        out=F.relu(out)\n",
    "        # Flatten node features across all nodes for each graph\n",
    "        out = out.reshape(-1,self.out_features)  # Shape: [batch_size, num_nodes * out_features[-1]]\n",
    "\n",
    "        # Pass through fully connected layer\n",
    "        out = F.relu(self.fc1(out))\n",
    "\n",
    "        out=F.relu(self.fc2(out))\n",
    "\n",
    "        return self.fc3(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[1/20]: train_loss=3.383, val_loss=0.718\n",
      "epoch[2/20]: train_loss=0.724, val_loss=0.701\n",
      "epoch[3/20]: train_loss=0.716, val_loss=0.760\n",
      "epoch[4/20]: train_loss=0.698, val_loss=0.699\n",
      "epoch[5/20]: train_loss=0.701, val_loss=0.697\n",
      "epoch[6/20]: train_loss=0.692, val_loss=0.709\n",
      "epoch[7/20]: train_loss=0.694, val_loss=0.730\n",
      "epoch[8/20]: train_loss=0.693, val_loss=0.707\n",
      "epoch[9/20]: train_loss=0.695, val_loss=0.701\n",
      "epoch[10/20]: train_loss=0.737, val_loss=0.731\n",
      "epoch[11/20]: train_loss=0.690, val_loss=0.712\n",
      "epoch[12/20]: train_loss=0.688, val_loss=0.753\n",
      "epoch[13/20]: train_loss=0.731, val_loss=0.738\n",
      "epoch[14/20]: train_loss=0.696, val_loss=0.718\n",
      "epoch[15/20]: train_loss=0.681, val_loss=0.718\n",
      "epoch[16/20]: train_loss=0.693, val_loss=0.735\n",
      "epoch[17/20]: train_loss=0.675, val_loss=0.746\n",
      "epoch[18/20]: train_loss=0.683, val_loss=0.727\n",
      "epoch[19/20]: train_loss=0.675, val_loss=0.704\n",
      "epoch[20/20]: train_loss=0.664, val_loss=0.726\n"
     ]
    }
   ],
   "source": [
    "\n",
    "heads=8\n",
    "out_features=16\n",
    "model2=GATClassifier(heads=heads,out_features=out_features)\n",
    "optimizer=torch.optim.Adam(model2.parameters(),lr=0.001)\n",
    "loss=nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs=20\n",
    "\n",
    "val_errors=[]\n",
    "train_errors=[]\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss_value=0\n",
    "    for batch in train_loader:\n",
    "\n",
    "        \n",
    "\n",
    "        output=model2(batch.x,batch.edge_index,batch.edge_attr,batch.batch)\n",
    "\n",
    "        l=loss(output,batch.y)\n",
    "\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_value+=l.detach().cpu().item()*len(batch)\n",
    "\n",
    "    train_errors.append(loss_value/len(train_set))\n",
    "\n",
    "    loss_value=0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in val_loader:\n",
    "\n",
    "            output=model2(batch.x,batch.edge_index,batch.edge_attr,batch.batch)\n",
    "\n",
    "            l=loss(output,batch.y)\n",
    "\n",
    "            loss_value+=l.cpu().item()*len(batch)\n",
    "\n",
    "        val_errors.append(loss_value/len(val_set))\n",
    "\n",
    "\n",
    "    print(f\"epoch[{epoch+1}/{n_epochs}]: train_loss={train_errors[epoch]:.3f}, val_loss={val_errors[epoch]:.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.9\n"
     ]
    }
   ],
   "source": [
    "model2.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_acc=0\n",
    "    for batch in test_loader:\n",
    "        output=model2(batch.x,batch.edge_index,batch.edge_attr,batch.batch)\n",
    "\n",
    "        test_acc+=(torch.argmax(output,axis=1)==batch.y).sum().item()\n",
    "\n",
    "test_acc=test_acc/len(test_set)\n",
    "\n",
    "print(np.round(test_acc*100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph Convolutional model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv,GlobalAttention,global_mean_pool\n",
    "\n",
    "\n",
    "class GCNClassifier(nn.Module):\n",
    "    def __init__(self, out_features=[32, 16,4], p=0.4, hidden_size1=16,hidden_size2=8,in_features=num_features, num_nodes=116):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_classes = 2\n",
    "        self.dropout = p\n",
    "        self.out_features=out_features\n",
    "        self.gate_nn=torch.nn.Linear(out_features[-1], 1)\n",
    "\n",
    "        # Define GCN layers\n",
    "        self.conv1 = GCNConv(in_features, out_features[0], normalize=False)\n",
    "        self.conv2 = GCNConv(out_features[0], out_features[1], normalize=False)\n",
    "        self.conv3=GCNConv(out_features[1], out_features[-1], normalize=False)\n",
    "        \n",
    "\n",
    "        # Fully connected layer for flattened node features\n",
    "        self.fc1 = nn.Linear(out_features[-1], hidden_size1)\n",
    "        self.fc2=nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3=nn.Linear(hidden_size2, self.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight,batch):\n",
    "        # GCN layers with ReLU activation\n",
    "        out = F.relu(self.conv1(x, edge_index, edge_weight))\n",
    "\n",
    "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "\n",
    "        out = F.relu(self.conv2(out, edge_index, edge_weight))\n",
    "\n",
    "        \n",
    "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "        \n",
    "        out=F.relu(self.conv3(out, edge_index, edge_weight))\n",
    "\n",
    "        \n",
    "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "\n",
    "\n",
    "        # Apply dropout\n",
    "        #global_attention_pool=GlobalAttention(self.gate_nn)\n",
    "        out = global_mean_pool(out,batch)\n",
    "        out=F.relu(out)\n",
    "        # Flatten node features across all nodes for each graph\n",
    "        out = out.reshape(-1,self.out_features[-1])  # Shape: [batch_size, num_nodes * out_features[-1]]\n",
    "\n",
    "        # Pass through fully connected layer\n",
    "        out = F.relu(self.fc1(out))\n",
    "\n",
    "        out=F.relu(self.fc2(out))\n",
    "\n",
    "        return self.fc3(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_features=[32,16,8]\n",
    "hidden_size1=8\n",
    "hidden_size2=4\n",
    "model=GCNClassifier(out_features=out_features, hidden_size1=hidden_size1,hidden_size2=hidden_size2)\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "loss=nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs=50\n",
    "\n",
    "val_errors=[]\n",
    "train_errors=[]\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss_value=0\n",
    "    for batch in train_loader:\n",
    "\n",
    "        \n",
    "\n",
    "        output=model(batch.x,batch.edge_index,batch.edge_attr,batch.batch)\n",
    "\n",
    "        l=loss(output,batch.y)\n",
    "\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_value+=l.detach().cpu().item()*len(batch)\n",
    "\n",
    "    train_errors.append(loss_value/len(train_set))\n",
    "\n",
    "    loss_value=0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in val_loader:\n",
    "\n",
    "            output=model(batch.x,batch.edge_index,batch.edge_attr,batch.batch)\n",
    "\n",
    "            l=loss(output,batch.y)\n",
    "\n",
    "            loss_value+=l.cpu().item()*len(batch)\n",
    "\n",
    "        val_errors.append(loss_value/len(val_set))\n",
    "\n",
    "\n",
    "    print(f\"epoch[{epoch+1}/{n_epochs}]: train_loss={train_errors[epoch]:.3f}, val_loss={val_errors[epoch]:.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy is :  52.7 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_acc=0\n",
    "    for batch in test_loader:\n",
    "        output=model(batch.x,batch.edge_index,batch.edge_attr,batch.batch)\n",
    "\n",
    "        test_acc+=(torch.argmax(output,axis=1)==batch.y).sum().item()\n",
    "\n",
    "test_acc=test_acc/len(test_set)\n",
    "\n",
    "print(\"test accuracy is : \",np.round(test_acc*100,1),\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
